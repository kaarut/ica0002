Last login: Sat Nov 20 23:03:25 on console
You have mail.
kareen@MBP-H8D ~ % ica
kareen@MBP-H8D ica0002 % 




















kareen@MBP-H8D ica0002 % nano hosts
kareen@MBP-H8D ica0002 % nano hosts
kareen@MBP-H8D ica0002 % run

PLAY [Setup] *******************************************************************

TASK [Gathering Facts] *********************************************************
ok: [vm2]
ok: [vm1]

TASK [setup] *******************************************************************
ok: [vm2]
ok: [vm1]

PLAY [DNS servers] *************************************************************

TASK [Gathering Facts] *********************************************************
ok: [vm2]

TASK [bind : Install bind9] ****************************************************
The following additional packages will be installed:
  bind9-dnsutils bind9-libs bind9-utils dns-root-data python3-ply
Suggested packages:
  bind-doc resolvconf python-ply-doc
The following NEW packages will be installed:
  bind9 bind9-utils dns-root-data python3-ply
The following packages will be upgraded:
  bind9-dnsutils bind9-libs
2 upgraded, 4 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [bind : Installing the options] *******************************************
--- before: /etc/bind/named.conf.options
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpf2hac1ac/named.conf.options.j2
@@ -1,24 +1,28 @@
 options {
 	directory "/var/cache/bind";
 
-	// If there is a firewall between you and nameservers you want
-	// to talk to, you may need to fix the firewall to allow multiple
-	// ports to talk.  See http://www.kb.cert.org/vuls/id/800113
+	forwarders {
+     
+        1.1.1.1;
+     
+        8.8.8.8;
+     
+        9.9.9.9;
+     
+        9.9.9.10;
+    	};
 
-	// If your ISP provided one or more IP addresses for stable 
-	// nameservers, you probably want to use them as forwarders.  
-	// Uncomment the following block, and insert the addresses replacing 
-	// the all-0's placeholder.
+	dnssec-validation no;
 
-	// forwarders {
-	// 	0.0.0.0;
-	// };
+    allow-query { 
+     
+        192.168.42.0/24;
+     
+        127.0.0.0/8;
+        };
+};
 
-	//========================================================================
-	// If BIND logs error messages about the root key being expired,
-	// you will need to update your keys.  See https://www.isc.org/bind-keys
-	//========================================================================
-	dnssec-validation auto;
+statistics-channels {
+  inet 127.0.0.1 port 8053 allow { 127.0.0.1; };
+};
 
-	listen-on-v6 { any; };
-};

changed: [vm2]

TASK [bind : Installing the local] *********************************************
--- before: /etc/bind/named.conf.local
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpp_ybq365/named.conf.local.j2
@@ -1,8 +1,4 @@
-//
-// Do any local configuration here
-//
-
-// Consider adding the 1918 zones here, if they are not used in your
-// organization
-//include "/etc/bind/zones.rfc1918";
-
+zone "lads.io" {
+    type primary;
+    file "db.lads";
+};

changed: [vm2]

TASK [bind : Installing the db] ************************************************
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmp_vzqj7it/db.lads.j2
@@ -0,0 +1,18 @@
+$TTL	604800
+lads.io.	IN	SOA	lads.io. kaarut.lads.io. (
+			      2		; Serial
+			 604800		; Refresh
+			  86400		; Retry
+			2419200		; Expire
+			 604800 )	; Negative Cache TTL
+;
+
+lads.io.		IN		NS		vm2.lads.io.
+
+
+ 
+vm1	IN		A		192.168.42.175
+ 
+vm2	IN		A		192.168.42.28
+
+backup  IN              A		192.168.42.156
\ No newline at end of file

changed: [vm2]

TASK [bind_exporter : Install bind9 exporter] **********************************
The following NEW packages will be installed:
  prometheus-bind-exporter
0 upgraded, 1 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [bind_exporter : Verify bind9 exporter is running] ************************
ok: [vm2]

TASK [dns_resolve : replace with new resolv file] ******************************
--- before: /etc/resolv.conf
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpkyubers7/resolv.conf.j2
@@ -1,19 +1,4 @@
-# This file is managed by man:systemd-resolved(8). Do not edit.
-#
-# This is a dynamic resolv.conf file for connecting local clients to the
-# internal DNS stub resolver of systemd-resolved. This file lists all
-# configured search domains.
-#
-# Run "resolvectl status" to see details about the uplink DNS servers
-# currently in use.
-#
-# Third party programs must not access this file directly, but only through the
-# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,
-# replace this symlink by a static file or a different symlink.
-#
-# See man:systemd-resolved.service(8) for details about the supported modes of
-# operation for /etc/resolv.conf.
+ 
+nameserver 192.168.42.28
 
-nameserver 127.0.0.53
-options edns0 trust-ad
-search openstacklocal
+search lads.io
\ No newline at end of file

changed: [vm2]

RUNNING HANDLER [bind : restart bind] ******************************************
changed: [vm2]

RUNNING HANDLER [bind : reload rndc] *******************************************
changed: [vm2]

RUNNING HANDLER [dns_resolve : restart dns_resolve] ****************************
changed: [vm2]

PLAY [Init] ********************************************************************

TASK [Gathering Facts] *********************************************************
ok: [vm2]
ok: [vm1]

TASK [init : Update APT cache] *************************************************
ok: [vm2]
ok: [vm1]

TASK [init : hostname] *********************************************************
--- before
+++ after
@@ -1 +1 @@
-hostname = kaarut-2
+hostname = vm2

changed: [vm2]
--- before
+++ after
@@ -1 +1 @@
-hostname = kaarut-1
+hostname = vm1

changed: [vm1]

TASK [backup : Add the user "backup" and generate keys] ************************
changed: [vm2]
changed: [vm1]

TASK [backup : ansible create directory] ***************************************
--- before
+++ after
@@ -1,5 +1,5 @@
 {
-    "owner": 0,
+    "owner": 34,
     "path": "/home/backup/restore",
-    "state": "absent"
+    "state": "directory"
 }

changed: [vm1]
--- before
+++ after
@@ -1,5 +1,5 @@
 {
-    "owner": 0,
+    "owner": 34,
     "path": "/home/backup/restore",
-    "state": "absent"
+    "state": "directory"
 }

changed: [vm2]

TASK [backup : Install duplicity] **********************************************
The following additional packages will be installed:
  librsync2 python3-bcrypt python3-fasteners python3-future python3-lockfile
  python3-monotonic python3-paramiko
Suggested packages:
  python3-boto ncftp lftp tahoe-lafs python3-swiftclient python3-pip par2
  python-future-doc python-lockfile-doc python3-gssapi
The following NEW packages will be installed:
  duplicity librsync2 python3-bcrypt python3-fasteners python3-future
  python3-lockfile python3-monotonic python3-paramiko
0 upgraded, 8 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]
The following additional packages will be installed:
  librsync2 python3-bcrypt python3-fasteners python3-future python3-lockfile
  python3-monotonic python3-paramiko
Suggested packages:
  python3-boto ncftp lftp tahoe-lafs python3-swiftclient python3-pip par2
  python-future-doc python-lockfile-doc python3-gssapi
The following NEW packages will be installed:
  duplicity librsync2 python3-bcrypt python3-fasteners python3-future
  python3-lockfile python3-monotonic python3-paramiko
0 upgraded, 8 newly installed, 0 to remove and 135 not upgraded.
changed: [vm1]

TASK [backup : Touch a ssh known hosts file] ***********************************
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmphq6ukkul/known_hosts.j2
@@ -0,0 +1,2 @@
+backup ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBSne71MCp6CQCHURWe3CLud6vPDkLfL83Oab/giAI7O
+backup.lads.io ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBSne71MCp6CQCHURWe3CLud6vPDkLfL83Oab/giAI7O

changed: [vm2]
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpkcaihotx/known_hosts.j2
@@ -0,0 +1,2 @@
+backup ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBSne71MCp6CQCHURWe3CLud6vPDkLfL83Oab/giAI7O
+backup.lads.io ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBSne71MCp6CQCHURWe3CLud6vPDkLfL83Oab/giAI7O

changed: [vm1]

TASK [docker : Docker package] *************************************************
The following additional packages will be installed:
  bridge-utils containerd dnsmasq-base libidn11 pigz python3-websocket runc
  ubuntu-fan
Suggested packages:
  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-doc
  rinse zfs-fuse | zfsutils
The following NEW packages will be installed:
  bridge-utils containerd dnsmasq-base docker.io libidn11 pigz python3-docker
  python3-websocket runc ubuntu-fan
0 upgraded, 10 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]
The following additional packages will be installed:
  bridge-utils containerd dns-root-data dnsmasq-base libidn11 pigz
  python3-websocket runc ubuntu-fan
Suggested packages:
  ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-doc
  rinse zfs-fuse | zfsutils
The following NEW packages will be installed:
  bridge-utils containerd dns-root-data dnsmasq-base docker.io libidn11 pigz
  python3-docker python3-websocket runc ubuntu-fan
0 upgraded, 11 newly installed, 0 to remove and 135 not upgraded.
changed: [vm1]

TASK [docker : Docker service] *************************************************
ok: [vm1]
ok: [vm2]

PLAY [MYSQL servers] ***********************************************************

TASK [mysql : Install MYSQL-server] ********************************************
The following additional packages will be installed:
  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-7
  libevent-pthreads-2.1-7 libfcgi-perl libhtml-parser-perl libhtml-tagset-perl
  libhtml-template-perl libhttp-date-perl libhttp-message-perl libio-html-perl
  liblwp-mediatypes-perl libmecab2 libtimedate-perl liburi-perl mecab-ipadic
  mecab-ipadic-utf8 mecab-utils mysql-client-8.0 mysql-client-core-8.0
  mysql-common mysql-server-8.0 mysql-server-core-8.0
Suggested packages:
  libdata-dump-perl libipc-sharedcache-perl libwww-perl mailx tinyca
  python-pymysql-doc
The following NEW packages will be installed:
  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-7
  libevent-pthreads-2.1-7 libfcgi-perl libhtml-parser-perl libhtml-tagset-perl
  libhtml-template-perl libhttp-date-perl libhttp-message-perl libio-html-perl
  liblwp-mediatypes-perl libmecab2 libtimedate-perl liburi-perl mecab-ipadic
  mecab-ipadic-utf8 mecab-utils mysql-client-8.0 mysql-client-core-8.0
  mysql-common mysql-server mysql-server-8.0 mysql-server-core-8.0
  python3-pymysql
0 upgraded, 26 newly installed, 0 to remove and 135 not upgraded.
changed: [vm1]
The following additional packages will be installed:
  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-7
  libevent-pthreads-2.1-7 libfcgi-perl libhtml-parser-perl libhtml-tagset-perl
  libhtml-template-perl libhttp-date-perl libhttp-message-perl libio-html-perl
  liblwp-mediatypes-perl libmecab2 libtimedate-perl liburi-perl mecab-ipadic
  mecab-ipadic-utf8 mecab-utils mysql-client-8.0 mysql-client-core-8.0
  mysql-common mysql-server-8.0 mysql-server-core-8.0
Suggested packages:
  libdata-dump-perl libipc-sharedcache-perl libwww-perl mailx tinyca
  python-pymysql-doc
The following NEW packages will be installed:
  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-7
  libevent-pthreads-2.1-7 libfcgi-perl libhtml-parser-perl libhtml-tagset-perl
  libhtml-template-perl libhttp-date-perl libhttp-message-perl libio-html-perl
  liblwp-mediatypes-perl libmecab2 libtimedate-perl liburi-perl mecab-ipadic
  mecab-ipadic-utf8 mecab-utils mysql-client-8.0 mysql-client-core-8.0
  mysql-common mysql-server mysql-server-8.0 mysql-server-core-8.0
  python3-pymysql
0 upgraded, 26 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [mysql : Replace the default mysql conf file] *****************************
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpf6tfl7_p/override.cnf.j2
@@ -0,0 +1,8 @@
+[mysqld]
+bind-address = 0.0.0.0
+
+# Replication conf
+log-bin = /var/log/mysql/mysql-bin.log
+relay-log = /var/log/mysql/mysql-relay.log
+replicate-do-db = agama
+server-id = 175

changed: [vm1]
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmp4gpf9al2/override.cnf.j2
@@ -0,0 +1,8 @@
+[mysqld]
+bind-address = 0.0.0.0
+
+# Replication conf
+log-bin = /var/log/mysql/mysql-bin.log
+relay-log = /var/log/mysql/mysql-relay.log
+replicate-do-db = agama
+server-id = 28

changed: [vm2]

TASK [mysql : start mysql server] **********************************************
ok: [vm1]
ok: [vm2]

TASK [mysql : MySQL database for Agama] ****************************************
changed: [vm2]
changed: [vm1]

TASK [mysql : MySQL user for Agama] ********************************************
changed: [vm1]
changed: [vm2]

TASK [mysql : MySQL user for replication] **************************************
changed: [vm1]
changed: [vm2]

TASK [mysql : Read only ON/OFF] ************************************************
changed: [vm2]
changed: [vm1]

TASK [mysql_exporter : MySQL user] *********************************************
changed: [vm1]
changed: [vm2]

TASK [mysql_exporter : Create config] ******************************************
fatal: [vm1]: FAILED! => {"changed": false, "checksum": "d47c6391b21ee42a19cbf4ef33dfc94297e685bf", "msg": "Destination directory /var/lib/prometheus does not exist"}
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpdkznza7x/my.cnf.j2
@@ -0,0 +1,3 @@
+[client]
+user=exporter
+password=123
\ No newline at end of file

changed: [vm2]

TASK [mysql_exporter : Install mysqld exporter] ********************************
The following additional packages will be installed:
  daemon
The following NEW packages will be installed:
  daemon prometheus-mysqld-exporter
0 upgraded, 2 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [mysql_exporter : Verify exporter is running] *****************************
ok: [vm2]

TASK [mysql_exporter : Change slave config] ************************************
--- before: /etc/default/prometheus-mysqld-exporter
+++ after: /Users/kareen/ica0002/roles/mysql_exporter/files/prometheus-mysqld-exporter
@@ -1,120 +1 @@
-# By default the connection string will be read from
-# $HOME/my.cnf or from the file specified with the -config.my-cnf parameter.
-
-# To set a connection string from the environment instead, uncomment one of the
-# following lines.
-
-# Using UNIX domain sockets and authentication:
-# DATA_SOURCE_NAME="prometheus:nopassword@unix(/run/mysqld/mysqld.sock)/"
-
-# Using a TCP connection and password authentication:
-# DATA_SOURCE_NAME="login:password@(hostname:port)/dbname"
-
-# Note the user must be granted enough privileges for the exporter to run.
-# Example to create a user to connect with the UNIX socket:
-#
-#  CREATE USER IF NOT EXISTS 'prometheus'@'localhost' IDENTIFIED VIA unix_socket;
-#  GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'prometheus'@'localhost';
-
-# Set the command-line arguments to pass to the exporter.
-# ARGS='-config.my-cnf /etc/mysql/debian.cnf'
-
-# Usage of prometheus-mysqld-exporter:
-#   -collect.auto_increment.columns
-#     	Collect auto_increment columns and max values from information_schema
-#  --exporter.lock_wait_timeout=2
-#       Set a lock_wait_timeout on the connection to avoid long metadata
-#       locking.
-#  --exporter.log_slow_filter
-#       Add a log_slow_filter to avoid slow query logging of scrapes. NOTE: Not
-#       supported by Oracle MySQL.
-#  --collect.heartbeat.database="heartbeat"
-#       Database from where to collect heartbeat data
-#  --collect.heartbeat.table="heartbeat"
-#       Table from where to collect heartbeat data
-#  --collect.info_schema.processlist.min_time=0
-#       Minimum time a thread must be in each state to be counted
-#  --collect.info_schema.tables.databases="*"
-#       The list of databases to collect table stats for, or '*' for all
-#  --collect.perf_schema.eventsstatements.limit=250
-#       Limit the number of events statements digests by response time
-#  --collect.perf_schema.eventsstatements.timelimit=86400
-#       Limit how old the 'last_seen' events statements can be, in seconds
-#  --collect.perf_schema.eventsstatements.digest_text_limit=120
-#       Maximum length of the normalized statement text
-#  --collect.perf_schema.file_instances.filter=".*"
-#       RegEx file_name filter for performance_schema.file_summary_by_instance
-#  --collect.perf_schema.file_instances.remove_prefix="/var/lib/mysql/"
-#       Remove path prefix in performance_schema.file_summary_by_instance
-#  --web.listen-address=":9104"
-#       Address to listen on for web interface and telemetry.
-#  --web.telemetry-path="/metrics"
-#       Path under which to expose metrics.
-#  --config.my-cnf="$HOME/.my.cnf"
-#       Path to .my.cnf file to read MySQL credentials from.
-#  --collect.global_variables
-#       Collect from SHOW GLOBAL VARIABLES
-#  --collect.slave_status
-#       Collect from SHOW SLAVE STATUS
-#  --collect.info_schema.processlist
-#       Collect current thread state counts from the
-#       information_schema.processlist
-#  --collect.info_schema.tables
-#       Collect metrics from information_schema.tables
-#  --collect.info_schema.innodb_tablespaces
-#       Collect metrics from information_schema.innodb_sys_tablespaces
-#  --collect.info_schema.innodb_metrics
-#       Collect metrics from information_schema.innodb_metrics
-#  --collect.auto_increment.columns
-#       Collect auto_increment columns and max values from information_schema
-#  --collect.global_status
-#       Collect from SHOW GLOBAL STATUS
-#  --collect.perf_schema.tableiowaits
-#       Collect metrics from performance_schema.table_io_waits_summary_by_table
-#  --collect.perf_schema.indexiowaits
-#       Collect metrics from
-#       performance_schema.table_io_waits_summary_by_index_usage
-#  --collect.perf_schema.tablelocks
-#       Collect metrics from
-#       performance_schema.table_lock_waits_summary_by_table
-#  --collect.perf_schema.eventsstatements
-#       Collect metrics from
-#       performance_schema.events_statements_summary_by_digest
-#  --collect.perf_schema.eventswaits
-#       Collect metrics from
-#       performance_schema.events_waits_summary_global_by_event_name
-#  --collect.perf_schema.file_events
-#       Collect metrics from performance_schema.file_summary_by_event_name
-#  --collect.perf_schema.file_instances
-#       Collect metrics from performance_schema.file_summary_by_instance
-#  --collect.binlog_size
-#       Collect the current size of all registered binlog files
-#  --collect.info_schema.userstats
-#       If running with userstat=1, set to true to collect user statistics
-#  --collect.info_schema.clientstats
-#       If running with userstat=1, set to true to collect client statistics
-#  --collect.info_schema.tablestats
-#       If running with userstat=1, set to true to collect table statistics
-#  --collect.info_schema.innodb_cmp
-#       Collect metrics from information_schema.innodb_cmp
-#  --collect.info_schema.innodb_cmpmem
-#       Collect metrics from information_schema.innodb_cmpmem
-#  --collect.info_schema.query_response_time
-#       Collect query response time distribution if query_response_time_stats
-#       is ON.
-#  --collect.engine_tokudb_status
-#       Collect from SHOW ENGINE TOKUDB STATUS
-#  --collect.perf_schema.replication_group_member_stats
-#       Collect metrics from performance_schema.replication_group_member_stats
-#  --collect.heartbeat
-#       Collect from heartbeat
-#  --collect.slave_hosts
-#       Scrape information from 'SHOW SLAVE HOSTS'
-#  --collect.engine_innodb_status
-#       Collect from SHOW ENGINE INNODB STATUS
-#  --log.level="info"
-#       Only log messages with the given severity or above. Valid levels:
-#       [debug, info, warn, error, fatal]
-#  --log.format="logger:stderr"
-#       Set the log target and format. Example:
-#       "logger:syslog?appname=bob&local=7" or "logger:stdout?json=true"
+ARGS="--collect.slave_status"

changed: [vm2]

TASK [mysql_backup : create directory] *****************************************
--- before
+++ after
@@ -1,6 +1,6 @@
 {
-    "mode": "0755",
-    "owner": 0,
+    "mode": "0700",
+    "owner": 34,
     "path": "/home/backup/mysql",
-    "state": "absent"
+    "state": "directory"
 }

changed: [vm2]

TASK [mysql_backup : Replace mysql conf file] **********************************
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpgcs7gjqt/my.cnf.j2
@@ -0,0 +1,6 @@
+[mysqldump]
+no_tablespaces
+
+[client]
+user=backup
+password=adminpass

changed: [vm2]

TASK [mysql_backup : MySQL user] ***********************************************
changed: [vm2]

TASK [mysql_backup : Make a cronjob] *******************************************
changed: [vm2]

RUNNING HANDLER [mysql : restart mysql] ****************************************
changed: [vm2]

RUNNING HANDLER [mysql : Reset MySQL source] ***********************************
ok: [vm2] => (item=stopreplica)
changed: [vm2] => (item=resetprimary)

RUNNING HANDLER [mysql : Reset MySQL replica] **********************************
skipping: [vm2] => (item=stopreplica) 
skipping: [vm2] => (item=changeprimary) 
skipping: [vm2] => (item=resetreplica) 
skipping: [vm2] => (item=startreplica) 

RUNNING HANDLER [mysql_exporter : restart mysqld_exporter] *********************
changed: [vm2]

PLAY [Web server] **************************************************************

PLAY [Exporters] ***************************************************************

TASK [Gathering Facts] *********************************************************
ok: [vm2]

TASK [node_exporter : Install APT package] *************************************
The following additional packages will be installed:
  libio-pty-perl libipc-run-perl libtime-duration-perl moreutils
  prometheus-node-exporter-collectors smartmontools
Suggested packages:
  gsmartcontrol smart-notifier mailx | mailutils
The following NEW packages will be installed:
  libio-pty-perl libipc-run-perl libtime-duration-perl moreutils
  prometheus-node-exporter prometheus-node-exporter-collectors smartmontools
0 upgraded, 7 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [node_exporter : Enable node exporter] ************************************
ok: [vm2]

TASK [nginx : Installing Nginx package] ****************************************
The following additional packages will be installed:
  fontconfig-config fonts-dejavu-core libfontconfig1 libgd3 libjbig0
  libjpeg-turbo8 libjpeg8 libnginx-mod-http-image-filter
  libnginx-mod-http-xslt-filter libnginx-mod-mail libnginx-mod-stream libtiff5
  libwebp6 libxpm4 nginx-common nginx-core
Suggested packages:
  libgd-tools fcgiwrap nginx-doc ssl-cert
The following NEW packages will be installed:
  fontconfig-config fonts-dejavu-core libfontconfig1 libgd3 libjbig0
  libjpeg-turbo8 libjpeg8 libnginx-mod-http-image-filter
  libnginx-mod-http-xslt-filter libnginx-mod-mail libnginx-mod-stream libtiff5
  libwebp6 libxpm4 nginx nginx-common nginx-core
0 upgraded, 17 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [nginx : Replace the default Nginx conf file] *****************************
--- before: /etc/nginx/sites-enabled/default
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmpn0usv4zs/default.j2
@@ -1,91 +1,48 @@
-##
-# You should look at the following URL's in order to grasp a solid understanding
-# of Nginx configuration files in order to fully unleash the power of Nginx.
-# https://www.nginx.com/resources/wiki/start/
-# https://www.nginx.com/resources/wiki/start/topics/tutorials/config_pitfalls/
-# https://wiki.debian.org/Nginx/DirectoryStructure
-#
-# In most cases, administrators will remove this file from sites-enabled/ and
-# leave it as reference inside of sites-available where it will continue to be
-# updated by the nginx packaging team.
-#
-# This file will automatically load configuration files provided by other
-# applications, such as Drupal or Wordpress. These applications will be made
-# available underneath a path with that package name, such as /drupal8.
-#
-# Please see /usr/share/doc/nginx-doc/examples/ for more detailed examples.
-##
+server {
+    listen 80 default_server;
+    server_name _;
+    
+    # WEB SERVERS 
+    
+    # NODE EXPORTER
+    location /node-metrics
+        {
+            proxy_pass http://localhost:9100/metrics;
+        }
 
-# Default server configuration
-#
+    # NGINX EXPORTER
+    location /nginx-metrics
+        {
+            proxy_pass http://localhost:9113/metrics;
+        }
+
+    # BIND EXPORTER 
+            location /bind-metrics  {
+            proxy_pass http://localhost:9119/metrics;
+        }
+
+    
+    # Databases
+            location /mysql-metrics  {
+            proxy_pass http://localhost:9104/metrics;
+        }
+    
+    # PROMETHEUS 
+    
+    # GRAFANA 
+    location /grafana
+        {
+            proxy_pass http://localhost:3001;
+        }
+
+    # InfluxDB
+    }
+
 server {
-	listen 80 default_server;
-	listen [::]:80 default_server;
+    listen 8080 default_server;
 
-	# SSL configuration
-	#
-	# listen 443 ssl default_server;
-	# listen [::]:443 ssl default_server;
-	#
-	# Note: You should disable gzip for SSL traffic.
-	# See: https://bugs.debian.org/773332
-	#
-	# Read up on ssl_ciphers to ensure a secure configuration.
-	# See: https://bugs.debian.org/765782
-	#
-	# Self signed certs generated by the ssl-cert package
-	# Don't use them in a production server!
-	#
-	# include snippets/snakeoil.conf;
-
-	root /var/www/html;
-
-	# Add index.php to the list if you are using PHP
-	index index.html index.htm index.nginx-debian.html;
-
-	server_name _;
-
-	location / {
-		# First attempt to serve request as file, then
-		# as directory, then fall back to displaying a 404.
-		try_files $uri $uri/ =404;
-	}
-
-	# pass PHP scripts to FastCGI server
-	#
-	#location ~ \.php$ {
-	#	include snippets/fastcgi-php.conf;
-	#
-	#	# With php-fpm (or other unix sockets):
-	#	fastcgi_pass unix:/var/run/php/php7.4-fpm.sock;
-	#	# With php-cgi (or other tcp sockets):
-	#	fastcgi_pass 127.0.0.1:9000;
-	#}
-
-	# deny access to .htaccess files, if Apache's document root
-	# concurs with nginx's one
-	#
-	#location ~ /\.ht {
-	#	deny all;
-	#}
-}
-
-
-# Virtual Host configuration for example.com
-#
-# You can move that to a different file under sites-available/ and symlink that
-# to sites-enabled/ to enable it.
-#
-#server {
-#	listen 80;
-#	listen [::]:80;
-#
-#	server_name example.com;
-#
-#	root /var/www/example.com;
-#	index index.html;
-#
-#	location / {
-#		try_files $uri $uri/ =404;
-#	}
-#}
+    # STUB
+    location = /stub_status {
+        stub_status;
+    }   
+}
\ No newline at end of file

changed: [vm2]

TASK [nginx : start nginx] *****************************************************
ok: [vm2]

TASK [nginx_exporter : Install nginx-exporter] *********************************
The following NEW packages will be installed:
  prometheus-nginx-exporter
0 upgraded, 1 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [nginx_exporter : Verify exporter is running] *****************************
changed: [vm2]

RUNNING HANDLER [nginx : restart nginx] ****************************************
changed: [vm2]

PLAY [Prometheus] **************************************************************

PLAY [Grafana] *****************************************************************

PLAY [Loggers] *****************************************************************

PLAY [influxdb_servers] ********************************************************

TASK [Gathering Facts] *********************************************************
ok: [vm2]

TASK [influxdb : Add an Apt signing key, uses whichever key is at the URL] *****
changed: [vm2]

TASK [influxdb : add this repo for stable release] *****************************
--- before: /dev/null
+++ after: /etc/apt/sources.list.d/repos_influxdata_com_ubuntu.list
@@ -0,0 +1 @@
+deb https://repos.influxdata.com/ubuntu focal stable

changed: [vm2]

TASK [influxdb : Install influxdb] *********************************************
The following NEW packages will be installed:
  influxdb
0 upgraded, 1 newly installed, 0 to remove and 133 not upgraded.
changed: [vm2]

TASK [influxdb : Change config] ************************************************
--- before: /etc/influxdb/influxdb.conf
+++ after: /Users/kareen/ica0002/roles/influxdb/files/influxdb.conf
@@ -1,596 +1,10 @@
-### Welcome to the InfluxDB configuration file.
-
-# The values in this file override the default values used by the system if
-# a config option is not specified. The commented out lines are the configuration
-# field and the default value used. Uncommenting a line and changing the value
-# will change the value used at runtime when the process is restarted.
-
-# Once every 24 hours InfluxDB will report usage data to usage.influxdata.com
-# The data includes a random ID, os, arch, version, the number of series and other
-# usage data. No data from user databases is ever transmitted.
-# Change this option to true to disable reporting.
-# reporting-disabled = false
-
-# Bind address to use for the RPC service for backup and restore.
-# bind-address = "127.0.0.1:8088"
-
-###
-### [meta]
-###
-### Controls the parameters for the Raft consensus group that stores metadata
-### about the InfluxDB cluster.
-###
-
 [meta]
-  # Where the metadata/raft database is stored
   dir = "/var/lib/influxdb/meta"
-
-  # Automatically create a default retention policy when creating a database.
-  # retention-autocreate = true
-
-  # If log messages are printed for the meta service
-  # logging-enabled = true
-
-###
-### [data]
-###
-### Controls where the actual shard data for InfluxDB lives and how it is
-### flushed from the WAL. "dir" may need to be changed to a suitable place
-### for your system, but the WAL settings are an advanced configuration. The
-### defaults should work for most systems.
-###
-
 [data]
-  # The directory where the TSM storage engine stores TSM files.
   dir = "/var/lib/influxdb/data"
-
-  # The directory where the TSM storage engine stores WAL files.
   wal-dir = "/var/lib/influxdb/wal"
-
-  # The amount of time that a write will wait before fsyncing.  A duration
-  # greater than 0 can be used to batch up multiple fsync calls.  This is useful for slower
-  # disks or when WAL write contention is seen.  A value of 0s fsyncs every write to the WAL.
-  # Values in the range of 0-100ms are recommended for non-SSD disks.
-  # wal-fsync-delay = "0s"
-
-
-  # The type of shard index to use for new shards.  The default is an in-memory index that is
-  # recreated at startup.  A value of "tsi1" will use a disk based index that supports higher
-  # cardinality datasets.
-  # index-version = "inmem"
-
-  # Trace logging provides more verbose output around the tsm engine. Turning
-  # this on can provide more useful output for debugging tsm engine issues.
-  # trace-logging-enabled = false
-
-  # Whether queries should be logged before execution. Very useful for troubleshooting, but will
-  # log any sensitive data contained within a query.
-  # query-log-enabled = true
-
-  # Provides more error checking. For example, SELECT INTO will err out inserting an +/-Inf value
-  # rather than silently failing.
-  # strict-error-handling = false
-
-  # Validates incoming writes to ensure keys only have valid unicode characters.
-  # This setting will incur a small overhead because every key must be checked.
-  # validate-keys = false
-
-  # Settings for the TSM engine
-
-  # CacheMaxMemorySize is the maximum size a shard's cache can
-  # reach before it starts rejecting writes.
-  # Valid size suffixes are k, m, or g (case insensitive, 1024 = 1k).
-  # Values without a size suffix are in bytes.
-  # cache-max-memory-size = "1g"
-
-  # CacheSnapshotMemorySize is the size at which the engine will
-  # snapshot the cache and write it to a TSM file, freeing up memory
-  # Valid size suffixes are k, m, or g (case insensitive, 1024 = 1k).
-  # Values without a size suffix are in bytes.
-  # cache-snapshot-memory-size = "25m"
-
-  # CacheSnapshotWriteColdDuration is the length of time at
-  # which the engine will snapshot the cache and write it to
-  # a new TSM file if the shard hasn't received writes or deletes
-  # cache-snapshot-write-cold-duration = "10m"
-
-  # CompactFullWriteColdDuration is the duration at which the engine
-  # will compact all TSM files in a shard if it hasn't received a
-  # write or delete
-  # compact-full-write-cold-duration = "4h"
-
-  # The maximum number of concurrent full and level compactions that can run at one time.  A
-  # value of 0 results in 50% of runtime.GOMAXPROCS(0) used at runtime.  Any number greater
-  # than 0 limits compactions to that value.  This setting does not apply
-  # to cache snapshotting.
-  # max-concurrent-compactions = 0
-
-  # CompactThroughput is the rate limit in bytes per second that we
-  # will allow TSM compactions to write to disk. Note that short bursts are allowed
-  # to happen at a possibly larger value, set by CompactThroughputBurst
-  # compact-throughput = "48m"
-
-  # CompactThroughputBurst is the rate limit in bytes per second that we
-  # will allow TSM compactions to write to disk.
-  # compact-throughput-burst = "48m"
-
-  # If true, then the mmap advise value MADV_WILLNEED will be provided to the kernel with respect to
-  # TSM files. This setting has been found to be problematic on some kernels, and defaults to off.
-  # It might help users who have slow disks in some cases.
-  # tsm-use-madv-willneed = false
-
-  # Settings for the inmem index
-
-  # The maximum series allowed per database before writes are dropped.  This limit can prevent
-  # high cardinality issues at the database level.  This limit can be disabled by setting it to
-  # 0.
-  # max-series-per-database = 1000000
-
-  # The maximum number of tag values per tag that are allowed before writes are dropped.  This limit
-  # can prevent high cardinality tag values from being written to a measurement.  This limit can be
-  # disabled by setting it to 0.
-  # max-values-per-tag = 100000
-
-  # Settings for the tsi1 index
-
-  # The threshold, in bytes, when an index write-ahead log file will compact
-  # into an index file. Lower sizes will cause log files to be compacted more
-  # quickly and result in lower heap usage at the expense of write throughput.
-  # Higher sizes will be compacted less frequently, store more series in-memory,
-  # and provide higher write throughput.
-  # Valid size suffixes are k, m, or g (case insensitive, 1024 = 1k).
-  # Values without a size suffix are in bytes.
-  # max-index-log-file-size = "1m"
-
-  # The size of the internal cache used in the TSI index to store previously 
-  # calculated series results. Cached results will be returned quickly from the cache rather
-  # than needing to be recalculated when a subsequent query with a matching tag key/value 
-  # predicate is executed. Setting this value to 0 will disable the cache, which may
-  # lead to query performance issues.
-  # This value should only be increased if it is known that the set of regularly used 
-  # tag key/value predicates across all measurements for a database is larger than 100. An
-  # increase in cache size may lead to an increase in heap usage.
   series-id-set-cache-size = 100
-
-###
-### [coordinator]
-###
-### Controls the clustering service configuration.
-###
-
-[coordinator]
-  # The default time a write request will wait until a "timeout" error is returned to the caller.
-  # write-timeout = "10s"
-
-  # The maximum number of concurrent queries allowed to be executing at one time.  If a query is
-  # executed and exceeds this limit, an error is returned to the caller.  This limit can be disabled
-  # by setting it to 0.
-  # max-concurrent-queries = 0
-
-  # The maximum time a query will is allowed to execute before being killed by the system.  This limit
-  # can help prevent run away queries.  Setting the value to 0 disables the limit.
-  # query-timeout = "0s"
-
-  # The time threshold when a query will be logged as a slow query.  This limit can be set to help
-  # discover slow or resource intensive queries.  Setting the value to 0 disables the slow query logging.
-  # log-queries-after = "0s"
-
-  # The maximum number of points a SELECT can process.  A value of 0 will make
-  # the maximum point count unlimited.  This will only be checked every second so queries will not
-  # be aborted immediately when hitting the limit.
-  # max-select-point = 0
-
-  # The maximum number of series a SELECT can run.  A value of 0 will make the maximum series
-  # count unlimited.
-  # max-select-series = 0
-
-  # The maximum number of group by time bucket a SELECT can create.  A value of zero will max the maximum
-  # number of buckets unlimited.
-  # max-select-buckets = 0
-
-###
-### [retention]
-###
-### Controls the enforcement of retention policies for evicting old data.
-###
-
-[retention]
-  # Determines whether retention policy enforcement enabled.
-  # enabled = true
-
-  # The interval of time when retention policy enforcement checks run.
-  # check-interval = "30m"
-
-###
-### [shard-precreation]
-###
-### Controls the precreation of shards, so they are available before data arrives.
-### Only shards that, after creation, will have both a start- and end-time in the
-### future, will ever be created. Shards are never precreated that would be wholly
-### or partially in the past.
-
-[shard-precreation]
-  # Determines whether shard pre-creation service is enabled.
-  # enabled = true
-
-  # The interval of time when the check to pre-create new shards runs.
-  # check-interval = "10m"
-
-  # The default period ahead of the endtime of a shard group that its successor
-  # group is created.
-  # advance-period = "30m"
-
-###
-### Controls the system self-monitoring, statistics and diagnostics.
-###
-### The internal database for monitoring data is created automatically if
-### if it does not already exist. The target retention within this database
-### is called 'monitor' and is also created with a retention period of 7 days
-### and a replication factor of 1, if it does not exist. In all cases the
-### this retention policy is configured as the default for the database.
-
-[monitor]
-  # Whether to record statistics internally.
-  # store-enabled = true
-
-  # The destination database for recorded statistics
-  # store-database = "_internal"
-
-  # The interval at which to record statistics
-  # store-interval = "10s"
-
-###
-### [http]
-###
-### Controls how the HTTP endpoints are configured. These are the primary
-### mechanism for getting data into and out of InfluxDB.
-###
-
+  query-log-enabled = false
 [http]
-  # Determines whether HTTP endpoint is enabled.
-  # enabled = true
-
-  # Determines whether the Flux query endpoint is enabled.
-  # flux-enabled = false
-
-  # Determines whether the Flux query logging is enabled.
-  # flux-log-enabled = false
-
-  # The bind address used by the HTTP service.
-  # bind-address = ":8086"
-
-  # Determines whether user authentication is enabled over HTTP/HTTPS.
-  # auth-enabled = false
-
-  # The default realm sent back when issuing a basic auth challenge.
-  # realm = "InfluxDB"
-
-  # Determines whether HTTP request logging is enabled.
-  # log-enabled = true
-
-  # Determines whether the HTTP write request logs should be suppressed when the log is enabled.
-  # suppress-write-log = false
-
-  # When HTTP request logging is enabled, this option specifies the path where
-  # log entries should be written. If unspecified, the default is to write to stderr, which
-  # intermingles HTTP logs with internal InfluxDB logging.
-  #
-  # If influxd is unable to access the specified path, it will log an error and fall back to writing
-  # the request log to stderr.
-  # access-log-path = ""
-
-  # Filters which requests should be logged. Each filter is of the pattern NNN, NNX, or NXX where N is
-  # a number and X is a wildcard for any number. To filter all 5xx responses, use the string 5xx.
-  # If multiple filters are used, then only one has to match. The default is to have no filters which
-  # will cause every request to be printed.
-  # access-log-status-filters = []
-
-  # Determines whether detailed write logging is enabled.
-  # write-tracing = false
-
-  # Determines whether the pprof endpoint is enabled.  This endpoint is used for
-  # troubleshooting and monitoring.
-  # pprof-enabled = true
-
-  # Enables authentication on pprof endpoints. Users will need admin permissions
-  # to access the pprof endpoints when this setting is enabled. This setting has
-  # no effect if either auth-enabled or pprof-enabled are set to false.
-  # pprof-auth-enabled = false
-
-  # Enables a pprof endpoint that binds to localhost:6060 immediately on startup.
-  # This is only needed to debug startup issues.
-  # debug-pprof-enabled = false
-
-  # Enables authentication on the /ping, /metrics, and deprecated /status
-  # endpoints. This setting has no effect if auth-enabled is set to false.
-  # ping-auth-enabled = false
-
-  # Determines whether HTTPS is enabled.
-  # https-enabled = false
-
-  # The SSL certificate to use when HTTPS is enabled.
-  # https-certificate = "/etc/ssl/influxdb.pem"
-
-  # Use a separate private key location.
-  # https-private-key = ""
-
-  # The JWT auth shared secret to validate requests using JSON web tokens.
-  # shared-secret = ""
-
-  # The default chunk size for result sets that should be chunked.
-  # max-row-limit = 0
-
-  # The maximum number of HTTP connections that may be open at once.  New connections that
-  # would exceed this limit are dropped.  Setting this value to 0 disables the limit.
-  # max-connection-limit = 0
-
-  # Enable http service over unix domain socket
-  # unix-socket-enabled = false
-
-  # The path of the unix domain socket.
-  # bind-socket = "/var/run/influxdb.sock"
-
-  # The maximum size of a client request body, in bytes. Setting this value to 0 disables the limit.
-  # max-body-size = 25000000
-
-  # The maximum number of writes processed concurrently.
-  # Setting this to 0 disables the limit.
-  # max-concurrent-write-limit = 0
-
-  # The maximum number of writes queued for processing.
-  # Setting this to 0 disables the limit.
-  # max-enqueued-write-limit = 0
-
-  # The maximum duration for a write to wait in the queue to be processed.
-  # Setting this to 0 or setting max-concurrent-write-limit to 0 disables the limit.
-  # enqueued-write-timeout = 0
-
-	# User supplied HTTP response headers
-	#
-	# [http.headers]
-	#   X-Header-1 = "Header Value 1"
-	#   X-Header-2 = "Header Value 2"
-
-###
-### [logging]
-###
-### Controls how the logger emits logs to the output.
-###
-
-[logging]
-  # Determines which log encoder to use for logs. Available options
-  # are auto, logfmt, and json. auto will use a more a more user-friendly
-  # output format if the output terminal is a TTY, but the format is not as
-  # easily machine-readable. When the output is a non-TTY, auto will use
-  # logfmt.
-  # format = "auto"
-
-  # Determines which level of logs will be emitted. The available levels
-  # are error, warn, info, and debug. Logs that are equal to or above the
-  # specified level will be emitted.
-  # level = "info"
-
-  # Suppresses the logo output that is printed when the program is started.
-  # The logo is always suppressed if STDOUT is not a TTY.
-  # suppress-logo = false
-
-###
-### [subscriber]
-###
-### Controls the subscriptions, which can be used to fork a copy of all data
-### received by the InfluxDB host.
-###
-
-[subscriber]
-  # Determines whether the subscriber service is enabled.
-  # enabled = true
-
-  # The default timeout for HTTP writes to subscribers.
-  # http-timeout = "30s"
-
-  # Allows insecure HTTPS connections to subscribers.  This is useful when testing with self-
-  # signed certificates.
-  # insecure-skip-verify = false
-
-  # The path to the PEM encoded CA certs file. If the empty string, the default system certs will be used
-  # ca-certs = ""
-
-  # The number of writer goroutines processing the write channel.
-  # write-concurrency = 40
-
-  # The number of in-flight writes buffered in the write channel.
-  # write-buffer-size = 1000
-
-
-###
-### [[graphite]]
-###
-### Controls one or many listeners for Graphite data.
-###
-
-[[graphite]]
-  # Determines whether the graphite endpoint is enabled.
-  # enabled = false
-  # database = "graphite"
-  # retention-policy = ""
-  # bind-address = ":2003"
-  # protocol = "tcp"
-  # consistency-level = "one"
-
-  # These next lines control how batching works. You should have this enabled
-  # otherwise you could get dropped metrics or poor performance. Batching
-  # will buffer points in memory if you have many coming in.
-
-  # Flush if this many points get buffered
-  # batch-size = 5000
-
-  # number of batches that may be pending in memory
-  # batch-pending = 10
-
-  # Flush at least this often even if we haven't hit buffer limit
-  # batch-timeout = "1s"
-
-  # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.
-  # udp-read-buffer = 0
-
-  ### This string joins multiple matching 'measurement' values providing more control over the final measurement name.
-  # separator = "."
-
-  ### Default tags that will be added to all metrics.  These can be overridden at the template level
-  ### or by tags extracted from metric
-  # tags = ["region=us-east", "zone=1c"]
-
-  ### Each template line requires a template pattern.  It can have an optional
-  ### filter before the template and separated by spaces.  It can also have optional extra
-  ### tags following the template.  Multiple tags should be separated by commas and no spaces
-  ### similar to the line protocol format.  There can be only one default template.
-  # templates = [
-  #   "*.app env.service.resource.measurement",
-  #   # Default template
-  #   "server.*",
-  # ]
-
-###
-### [collectd]
-###
-### Controls one or many listeners for collectd data.
-###
-
-[[collectd]]
-  # enabled = false
-  # bind-address = ":25826"
-  # database = "collectd"
-  # retention-policy = ""
-  #
-  # The collectd service supports either scanning a directory for multiple types
-  # db files, or specifying a single db file.
-  # typesdb = "/usr/local/share/collectd"
-  #
-  # security-level = "none"
-  # auth-file = "/etc/collectd/auth_file"
-
-  # These next lines control how batching works. You should have this enabled
-  # otherwise you could get dropped metrics or poor performance. Batching
-  # will buffer points in memory if you have many coming in.
-
-  # Flush if this many points get buffered
-  # batch-size = 5000
-
-  # Number of batches that may be pending in memory
-  # batch-pending = 10
-
-  # Flush at least this often even if we haven't hit buffer limit
-  # batch-timeout = "10s"
-
-  # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.
-  # read-buffer = 0
-
-  # Multi-value plugins can be handled two ways.
-  # "split" will parse and store the multi-value plugin data into separate measurements
-  # "join" will parse and store the multi-value plugin as a single multi-value measurement.
-  # "split" is the default behavior for backward compatibility with previous versions of influxdb.
-  # parse-multivalue-plugin = "split"
-###
-### [opentsdb]
-###
-### Controls one or many listeners for OpenTSDB data.
-###
-
-[[opentsdb]]
-  # enabled = false
-  # bind-address = ":4242"
-  # database = "opentsdb"
-  # retention-policy = ""
-  # consistency-level = "one"
-  # tls-enabled = false
-  # certificate= "/etc/ssl/influxdb.pem"
-
-  # Log an error for every malformed point.
-  # log-point-errors = true
-
-  # These next lines control how batching works. You should have this enabled
-  # otherwise you could get dropped metrics or poor performance. Only points
-  # metrics received over the telnet protocol undergo batching.
-
-  # Flush if this many points get buffered
-  # batch-size = 1000
-
-  # Number of batches that may be pending in memory
-  # batch-pending = 5
-
-  # Flush at least this often even if we haven't hit buffer limit
-  # batch-timeout = "1s"
-
-###
-### [[udp]]
-###
-### Controls the listeners for InfluxDB line protocol data via UDP.
-###
-
-[[udp]]
-  # enabled = false
-  # bind-address = ":8089"
-  # database = "udp"
-  # retention-policy = ""
-
-  # InfluxDB precision for timestamps on received points ("" or "n", "u", "ms", "s", "m", "h")
-  # precision = ""
-
-  # These next lines control how batching works. You should have this enabled
-  # otherwise you could get dropped metrics or poor performance. Batching
-  # will buffer points in memory if you have many coming in.
-
-  # Flush if this many points get buffered
-  # batch-size = 5000
-
-  # Number of batches that may be pending in memory
-  # batch-pending = 10
-
-  # Will flush at least this often even if we haven't hit buffer limit
-  # batch-timeout = "1s"
-
-  # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.
-  # read-buffer = 0
-
-###
-### [continuous_queries]
-###
-### Controls how continuous queries are run within InfluxDB.
-###
-
-[continuous_queries]
-  # Determines whether the continuous query service is enabled.
-  # enabled = true
-
-  # Controls whether queries are logged when executed by the CQ service.
-  # log-enabled = true
-
-  # Controls whether queries are logged to the self-monitoring data store.
-  # query-stats-enabled = false
-
-  # interval for how often continuous queries will be checked if they need to run
-  # run-interval = "1s"
-
-###
-### [tls]
-###
-### Global configuration settings for TLS in InfluxDB.
-###
-
-[tls]
-  # Determines the available set of cipher suites. See https://golang.org/pkg/crypto/tls/#pkg-constants
-  # for a list of available ciphers, which depends on the version of Go (use the query
-  # SHOW DIAGNOSTICS to see the version of Go used to build InfluxDB). If not specified, uses
-  # the default settings from Go's crypto/tls package.
-  # ciphers = [
-  #   "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
-  #   "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
-  # ]
-
-  # Minimum version of the tls protocol that will be negotiated. If not specified, uses the
-  # default settings from Go's crypto/tls package.
-  # min-version = "tls1.2"
-
-  # Maximum version of the tls protocol that will be negotiated. If not specified, uses the
-  # default settings from Go's crypto/tls package.
-  # max-version = "tls1.3"
+  log-enabled = false
+  write-tracing = false

changed: [vm2]

TASK [influxdb : Verify influxdb is running] ***********************************
changed: [vm2]

TASK [influxdb_backup : create influxdb dir] ***********************************
--- before
+++ after
@@ -1,6 +1,6 @@
 {
-    "mode": "0755",
-    "owner": 0,
+    "mode": "0700",
+    "owner": 34,
     "path": "/home/backup/influxdb",
-    "state": "absent"
+    "state": "directory"
 }

changed: [vm2]

TASK [influxdb_backup : create restore dir] ************************************
--- before
+++ after
@@ -1,4 +1,4 @@
 {
-    "mode": "0755",
+    "mode": "0700",
     "path": "/home/backup/restore"
 }

changed: [vm2]

TASK [influxdb_backup : Make a cronjob] ****************************************
--- before
+++ after: /Users/kareen/.ansible/tmp/ansible-local-2322jqku0z8x/tmp9n4dyyk_/influxdb-backup
@@ -0,0 +1,3 @@
+10 */11 * * * backup rm -rf /home/backup/influxdb/*; influxd backup -database telegraf /home/backup/influxdb
+0 11 * * * backup duplicity --no-encryption full /home/backup/influxdb rsync://kaarut@backup//home/kaarut/
+20 */11 * * * backup duplicity --no-encryption incremental /home/backup/influxdb rsync://kaarut@backup//home/kaarut/

changed: [vm2]

RUNNING HANDLER [influxdb : restart influx] ************************************
changed: [vm2]

PLAY RECAP *********************************************************************
vm1                        : ok=19   changed=13   unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
vm2                        : ok=60   changed=47   unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   

kareen@MBP-H8D ica0002 % 

